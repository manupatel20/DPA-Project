


# Install required packages 
```{r}
install.packages(c("dplyr", "tidytext", "stringr", "ggplot2", "tm", "textclean", "wordcloud", "tidyr"))
```

# Load Libraries
```{r}
library(dplyr)
library(tidytext)
library(stringr)
library(ggplot2)
library(tm)
library(textclean)
library(wordcloud)
library(tidyr)
```

# Load Dataset
```{r}
sentiment_data <- read.csv("sentiment140.csv", 
                           header = FALSE, 
                           col.names = c("sentiment", "id", "date", "flag", "user", "text"),
                           encoding = "UTF-8",
                           stringsAsFactors = FALSE)
```

# Display initial data structure
```{r}
cat("Original dataset dimensions:", dim(sentiment_data), "\n")
cat("Sample tweets:\n")
head(sentiment_data$text, 3)
```
# Step 1: Filtering only EV-related tweets
# Creating a comprehensive list of EV-related term
```{r}
ev_terms <- c(
  # EV general terms
  "electric vehicle", "electric car", "ev ", "evs ", "battery electric", "plugin", "plug-in",
  "charging station", "charging port", "charging network", "charge point", "supercharger",
  "range anxiety", "zero emission", "tailpipe emission", "carbon neutral", "kwh", "kilowatt hour",
  
  # EV brands and models
  "tesla", "model 3", "model s", "model x", "model y", "cybertruck", 
  "nissan leaf", "chevy bolt", "chevrolet bolt", "bolt ev", "rivian", "lucid air",
  "polestar", "kia ev6", "hyundai ioniq", "volkswagen id", "ford mustang mach", "f-150 lightning",
  "audi e-tron", "porsche taycan", "bmw i3", "bmw i4", "bmw ix", "jaguar i-pace",
  "mercedes eqc", "volvo xc40 recharge", "mini cooper se", "hummer ev",
  
  # Charging infrastructure
  "evgo", "chargepoint", "electrify america", "blink charging", "level 2 charger", "dcfc",
  "ccs connector", "chademo", "j1772", "fast charger", "home charger", "destination charger",
  
  # Policy terms
  "ev incentive", "ev tax credit", "ev rebate", "clean vehicle", "green vehicle"
)
```

# Function to check if tweet contains any EV-related term
```{r}
contains_ev_terms <- function(text) {
  text <- tolower(text)
  any(sapply(ev_terms, function(term) str_detect(text, fixed(term))))
}
```

# Filter tweets that contain EV-related terms
```{r}

# Convert text column to UTF-8
sentiment_data$text <- iconv(sentiment_data$text, from = "latin1", to = "UTF-8", sub = "")

ev_tweets <- sentiment_data %>%
  filter(sapply(text, contains_ev_terms))


cat("EV-related tweets found:", nrow(ev_tweets), "\n")
```

# Step 2: Text preprocessing
```{r}
preprocess_tweet <- function(tweet) {
  # Convert to lowercase
  tweet <- tolower(tweet)
  
  # Remove URLs
  tweet <- str_replace_all(tweet, "http\\S+|www\\S+", "")
  
  # Remove usernames (handles)
  tweet <- str_replace_all(tweet, "@\\w+", "")
  
  # Remove hashtag symbol (but keep the word)
  tweet <- str_replace_all(tweet, "#(\\w+)", "\\1")
  
  # Remove RT tag
  tweet <- str_replace_all(tweet, "^rt ", "")
  
  # Remove non-alphanumeric characters and extra whitespace
  tweet <- str_replace_all(tweet, "[^[:alnum:][:space:]']", " ")
  tweet <- str_replace_all(tweet, "\\s+", " ")
  
  # Remove leading and trailing whitespace
  tweet <- str_trim(tweet)
  
  return(tweet)
}

# Apply preprocessing to all tweets
ev_tweets$cleaned_text <- sapply(ev_tweets$text, preprocess_tweet)
```

# Step 3: Tokenization, stopword removal, and lemmatization
```{r}
# Load stopwords
data(stop_words)

# Add custom stopwords that might be common in tweets but not informative
custom_stopwords <- tibble(
  word = c("amp", "rt", "via", "just", "like", "will", "can", "now", "get", "got", "one", "im", "say", "said", "today", "day", "dont"),
  lexicon = "custom"
)

all_stopwords <- bind_rows(stop_words, custom_stopwords)

# Tokenize, remove stopwords, and lemmatize
ev_tweets_processed <- ev_tweets %>%
  unnest_tokens(word, cleaned_text) %>%
  anti_join(all_stopwords, by = "word") %>%
  filter(nchar(word) > 1)  # Remove single-character tokens

# Using the textstem package for lemmatization
# Note: Installing and loading the textstem package is necessary for this step
if (!requireNamespace("textstem", quietly = TRUE)) {
  cat("Installing textstem package...\n")
  install.packages("textstem")
}
library(textstem)

# Group by tweet ID to lemmatize each tweet's words
lemmatized_tweets <- ev_tweets_processed %>%
  group_by(id) %>%
  mutate(lemma = lemmatize_words(word)) %>%
  ungroup()

# Reconstruct tweets after processing
processed_tweets <- lemmatized_tweets %>%
  group_by(id) %>%
  summarise(
    processed_text = paste(lemma, collapse = " "),
    original_text = first(text),
    sentiment = first(sentiment),
    date = first(date),
    user = first(user)
  )
```
# Step 4: Convert sentiment labels
```{r}
processed_tweets <- processed_tweets %>%
  mutate(sentiment_label = case_when(
    sentiment == 0 ~ "Negative",
    sentiment == 2 ~ "Neutral",
    sentiment == 4 ~ "Positive",
    TRUE ~ "Unknown"
  ))
```

# Step 6: Save the cleaned dataset
```{r}
write.csv(processed_tweets, "cleaned_ev_tweets.csv", row.names = FALSE)
cat("Cleaned dataset saved as 'cleaned_ev_tweets.csv'\n")
```

